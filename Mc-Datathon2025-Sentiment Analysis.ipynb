{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c7d817",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Configuration initiale\n",
    "device = torch.device(\"cpu\")  # Forcé sur CPU pour éviter les erreurs CUDA\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Chargement et nettoyage des données\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Limiter la taille pour accélérer les tests (commenter pour dataset complet)\n",
    "# train_df = train_df.head(1000)\n",
    "# test_df = test_df.head(1000)\n",
    "\n",
    "def clean_text(text):\n",
    "    return ' '.join(str(text).split()).strip() if pd.notna(text) else \"neutral\"\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "# Validation des données\n",
    "print(\"Train columns:\", train_df.columns)\n",
    "print(\"Test columns:\", test_df.columns)\n",
    "print(\"NaN in train_df['text']:\", train_df['text'].isna().sum())\n",
    "print(\"NaN in train_df['sentiment']:\", train_df['sentiment'].isna().sum())\n",
    "\n",
    "# Mapping des sentiments\n",
    "sentiment_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "print(\"Unique sentiments before mapping:\", train_df['sentiment'].unique())\n",
    "train_df['label'] = train_df['sentiment'].map(sentiment_map)\n",
    "\n",
    "# Supprimer les lignes avec des étiquettes invalides\n",
    "invalid_rows = train_df[train_df['label'].isna()]\n",
    "if not invalid_rows.empty:\n",
    "    print(\"Invalid sentiments found:\", invalid_rows['sentiment'].unique())\n",
    "    train_df = train_df.dropna(subset=['label'])\n",
    "    print(\"Removed invalid rows. New train_df size:\", len(train_df))\n",
    "\n",
    "# Validation des étiquettes après nettoyage\n",
    "print(\"Unique labels after mapping:\", train_df['label'].unique())\n",
    "if not all(train_df['label'].isin([0, 1, 2])):\n",
    "    raise ValueError(\"Labels must be in [0, 1, 2]\")\n",
    "\n",
    "\n",
    "# Tokenization avec DistilBERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def encode_texts(texts, max_len=64):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Encodage\n",
    "encoded_train = encode_texts(train_df['text'].tolist())\n",
    "input_ids = encoded_train['input_ids']\n",
    "attention_masks = encoded_train['attention_mask']\n",
    "labels = torch.tensor(train_df['label'].values, dtype=torch.long)\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "# Préparation des DataLoaders\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)  # Réduit pour CPU\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "# Initialisation du modèle\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=3,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Configuration de l'entraînement\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 2  # Réduit pour accélérer\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs, masks, lbls = batch\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=masks, labels=lbls)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Perte moyenne: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "val_accuracy = 0\n",
    "for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    inputs, masks, lbls = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs, attention_mask=masks, labels=lbls)\n",
    "    val_loss += outputs.loss.item()\n",
    "    preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "    val_accuracy += (preds == lbls.cpu().numpy()).mean()\n",
    "\n",
    "print(f\"Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy/len(val_loader):.4f}\")\n",
    "\n",
    "# Prédiction\n",
    "def predict(texts):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    encoded = encode_texts(texts)\n",
    "    pred_loader = DataLoader(TensorDataset(encoded['input_ids'], encoded['attention_mask']), batch_size=4)\n",
    "\n",
    "    for batch in tqdm(pred_loader, desc=\"Predicting\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs, masks = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, attention_mask=masks)\n",
    "\n",
    "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "    return [['negative', 'neutral', 'positive'][p] for p in predictions]\n",
    "\n",
    "# Génération des résultats\n",
    "test_predictions = predict(test_df['text'].tolist())\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'sentiment': test_predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Prédictions sauvegardées dans submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
