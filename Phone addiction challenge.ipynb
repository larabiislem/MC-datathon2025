{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dbcb6f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as mp\n",
    "import gender_guesser.detector as gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87426bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "test_ids = test['id'].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d = gender.Detector()\n",
    "df = train.copy()\n",
    "df_test = test.copy()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "def impute_gender(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Gender'] == 'Prefer not to say':\n",
    "            prenom = row['Name'].split()[0].capitalize()\n",
    "            guess = d.get_gender(prenom)\n",
    "            if 'female' in guess:\n",
    "                df.at[index, 'Gender'] = 'Female'\n",
    "            elif 'male' in guess:\n",
    "                df.at[index, 'Gender'] = 'Male'\n",
    "            else:\n",
    "                df.at[index, 'Gender'] = 'Female'\n",
    "    return df\n",
    "\n",
    "df = impute_gender(df)\n",
    "df_test = impute_gender(df_test)\n",
    "df[\"Mean_Age_by_Location\"] = df.groupby(\"Location\")[\"Age\"].transform(\"mean\")\n",
    "df_test[\"Mean_Age_by_Location\"] = df_test.groupby(\"Location\")[\"Age\"].transform(\"mean\")\n",
    "# Exemples de features supplémentaires liées à Location\n",
    "df[\"Count_by_Location\"] = df.groupby(\"Location\")[\"Age\"].transform(\"count\")\n",
    "df_test[\"Count_by_Location\"] = df_test.groupby(\"Location\")[\"Age\"].transform(\"count\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train = df.drop(['id', 'Name', 'Location'], axis=1)\n",
    "test = df_test.drop(['id', 'Name', 'Location'], axis=1)\n",
    "\n",
    "# Separate target\n",
    "X_train = train.drop('Addiction_Level', axis=1)\n",
    "y_train = train['Addiction_Level']\n",
    "\n",
    "# Handle categorical variables\n",
    "cat_cols = ['Gender', 'Phone_Usage_Purpose', 'School_Grade']\n",
    "num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "num_cols = [col for col in num_cols if col not in cat_cols]\n",
    "\n",
    "# Enhanced feature engineering\n",
    "def create_features(df):\n",
    "    # Core interactions\n",
    "    df['Mental_Health_Index'] = (df['Anxiety_Level'] * 0.6 + df['Depression_Level'] * 0.4)\n",
    "    df['Total_Screen_Time'] = df['Time_on_Social_Media'] + df['Time_on_Gaming'] + df['Time_on_Education']\n",
    "    \n",
    "    # Advanced interactions\n",
    "    df['Usage_Mental_Health'] = df['Daily_Usage_Hours'] * df['Mental_Health_Index']\n",
    "    df['Sleep_Quality'] = df['Sleep_Hours'] / (df['Screen_Time_Before_Bed'] + 1)\n",
    "    df['Productivity_Ratio'] = df['Time_on_Education'] / (df['Total_Screen_Time'] + 0.01)\n",
    "    \n",
    "    # Non-linear transformations\n",
    "    df['Log_Usage'] = np.log1p(df['Daily_Usage_Hours'])\n",
    "    df['Sqrt_Anxiety'] = np.sqrt(df['Anxiety_Level'])\n",
    "    df['Exp_Sleep'] = np.expm1(df['Sleep_Hours'] * 0.1)\n",
    "    \n",
    "    # Behavioral patterns\n",
    "    df['Night_Owl'] = (df['Screen_Time_Before_Bed'] > 2).astype(int)\n",
    "    df['Social_Dominant'] = (df['Time_on_Social_Media'] > df['Time_on_Gaming']).astype(int)\n",
    "    \n",
    "    # Binning with interactions\n",
    "    df['Usage_Bin'] = pd.qcut(df['Daily_Usage_Hours'], q=5, labels=False)\n",
    "    df['Sleep_Bin'] = pd.qcut(df['Sleep_Hours'], q=5, labels=False)\n",
    "    df['Usage_Sleep_Interaction'] = df['Usage_Bin'] * df['Sleep_Bin']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "X_train = create_features(X_train)\n",
    "test = create_features(test)\n",
    "\n",
    "# Ensure test has all columns from train\n",
    "for col in X_train.columns:\n",
    "    if col not in test.columns:\n",
    "        test[col] = 0\n",
    "\n",
    "test = test[X_train.columns]\n",
    "\n",
    "# Enhanced preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('transformer', PowerTransformer()),\n",
    "    ('selector', SelectFromModel(LGBMRegressor(), threshold='1.25*mean'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ])\n",
    "\n",
    "# Optimized base models (CPU-only)\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=2000,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.015,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    gamma=0.2,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=0.2,\n",
    "    random_state=42,\n",
    "    tree_method='hist'  # Changed from GPU to CPU method\n",
    ")\n",
    "\n",
    "lgbm_model = LGBMRegressor(\n",
    "    n_estimators=2000,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.02,\n",
    "    num_leaves=50,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=0.2,\n",
    "    random_state=42,\n",
    "    force_row_wise=True  # Added for stability\n",
    ")\n",
    "\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=2000,\n",
    "    depth=6,\n",
    "    learning_rate=0.025,\n",
    "    l2_leaf_reg=0.5,\n",
    "    random_seed=42,\n",
    "    verbose=0,\n",
    "    task_type='CPU'  # Explicitly set to CPU\n",
    ")\n",
    "\n",
    "# Two-level stacking approach\n",
    "first_level = [\n",
    "    ('xgb', xgb_model),\n",
    "    ('lgbm', lgbm_model),\n",
    "    ('cat', cat_model)\n",
    "]\n",
    "\n",
    "meta_models = {\n",
    "    'hgb': HistGradientBoostingRegressor(\n",
    "        max_iter=300, \n",
    "        max_depth=5, \n",
    "        learning_rate=0.05, \n",
    "        random_state=42\n",
    "    ),\n",
    "    'kernel': KernelRidge(\n",
    "        alpha=0.5, \n",
    "        kernel='polynomial', \n",
    "        degree=3, \n",
    "        coef0=3\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create stacking models\n",
    "stack_models = {}\n",
    "for name, meta_model in meta_models.items():\n",
    "    stack = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('stack', StackingRegressor(\n",
    "            estimators=first_level,\n",
    "            final_estimator=meta_model,\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            passthrough=True\n",
    "        ))\n",
    "    ])\n",
    "    stack_models[name] = stack\n",
    "\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train, test_size=0.05, random_state=50\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "predictions = []\n",
    "weights = []\n",
    "for name, model in stack_models.items():\n",
    "    model.fit(X_train_split, y_train_split)\n",
    "    val_pred = model.predict(X_val_split)\n",
    "    mse = mean_squared_error(y_val_split, val_pred)\n",
    "    print(f\"{name} MSE: {mse:.6f}\")\n",
    "    predictions.append(model.predict(test))\n",
    "    weights.append(1 / mse)\n",
    "\n",
    "# Weighted ensemble\n",
    "weights = np.array(weights) / sum(weights)\n",
    "y_pred_test = np.sum([w * p for w, p in zip(weights, predictions)], axis=0)\n",
    "\n",
    "# Full training with best model\n",
    "best_model_name = min(stack_models.keys(), key=lambda x: weights[list(stack_models.keys()).index(x)])\n",
    "best_model = stack_models[best_model_name]\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Final prediction blending\n",
    "final_pred = best_model.predict(test)\n",
    "y_pred_test = 0.2* final_pred + 0.8 * y_pred_test\n",
    "\n",
    "# Post-processing\n",
    "y_pred_test = np.clip(y_pred_test, y_train.min(), y_train.max())\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, \n",
    "                          cv=5, scoring='neg_mean_squared_error')\n",
    "final_mse = -np.mean(cv_scores)\n",
    "print(f\"\\nFinal CV MSE: {final_mse:.6f}\")\n",
    "\n",
    "# Submission\n",
    "submission = pd.DataFrame({'id': test_ids, 'Addiction_Level': y_pred_test})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file generated\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
